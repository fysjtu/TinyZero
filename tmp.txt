2025-11-19 23:30:22,294	WARNING utils.py:460 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-11-19 23:30:22,464	INFO worker.py:2012 -- Started a local Ray instance.
/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
[36m(main_task pid=34262)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=34262)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=34262)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=34262)[0m                                                  'grad_offload': False,
[36m(main_task pid=34262)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=34262)[0m                                                  'param_offload': False,
[36m(main_task pid=34262)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=34262)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=34262)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=34262)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=34262)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=34262)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=34262)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=34262)[0m                                            'total_training_steps': -1,
[36m(main_task pid=34262)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=34262)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=34262)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=34262)[0m                                  'ppo_micro_batch_size': 2,
[36m(main_task pid=34262)[0m                                  'ppo_mini_batch_size': 8,
[36m(main_task pid=34262)[0m                                  'shuffle': False,
[36m(main_task pid=34262)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=34262)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=34262)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=34262)[0m                                  'use_kl_loss': False},
[36m(main_task pid=34262)[0m                        'hybrid_engine': True,
[36m(main_task pid=34262)[0m                        'model': {'enable_gradient_checkpointing': False,
[36m(main_task pid=34262)[0m                                  'external_lib': None,
[36m(main_task pid=34262)[0m                                  'override_config': {},
[36m(main_task pid=34262)[0m                                  'path': '/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B',
[36m(main_task pid=34262)[0m                                  'use_remove_padding': True},
[36m(main_task pid=34262)[0m                        'ref': {'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=34262)[0m                                                'param_offload': False,
[36m(main_task pid=34262)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=34262)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=34262)[0m                                'log_prob_micro_batch_size': 1,
[36m(main_task pid=34262)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=34262)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=34262)[0m                        'rollout': {'do_sample': True,
[36m(main_task pid=34262)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=34262)[0m                                    'enforce_eager': True,
[36m(main_task pid=34262)[0m                                    'free_cache_engine': True,
[36m(main_task pid=34262)[0m                                    'gpu_memory_utilization': 0.4,
[36m(main_task pid=34262)[0m                                    'ignore_eos': False,
[36m(main_task pid=34262)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=34262)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=34262)[0m                                    'log_prob_micro_batch_size': 1,
[36m(main_task pid=34262)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=34262)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=34262)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=34262)[0m                                    'n': 1,
[36m(main_task pid=34262)[0m                                    'name': 'vllm',
[36m(main_task pid=34262)[0m                                    'prompt_length': 256,
[36m(main_task pid=34262)[0m                                    'response_length': 512,
[36m(main_task pid=34262)[0m                                    'temperature': 1.0,
[36m(main_task pid=34262)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=34262)[0m                                    'top_k': -1,
[36m(main_task pid=34262)[0m                                    'top_p': 1}},
[36m(main_task pid=34262)[0m  'algorithm': {'adv_estimator': 'gae',
[36m(main_task pid=34262)[0m                'gamma': 1.0,
[36m(main_task pid=34262)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=34262)[0m                'kl_penalty': 'kl',
[36m(main_task pid=34262)[0m                'lam': 1.0},
[36m(main_task pid=34262)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=34262)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=34262)[0m             'forward_micro_batch_size': 2,
[36m(main_task pid=34262)[0m             'grad_clip': 1.0,
[36m(main_task pid=34262)[0m             'model': {'enable_gradient_checkpointing': False,
[36m(main_task pid=34262)[0m                       'external_lib': None,
[36m(main_task pid=34262)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=34262)[0m                                       'grad_offload': False,
[36m(main_task pid=34262)[0m                                       'optimizer_offload': False,
[36m(main_task pid=34262)[0m                                       'param_offload': False,
[36m(main_task pid=34262)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=34262)[0m                       'override_config': {},
[36m(main_task pid=34262)[0m                       'path': '/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B',
[36m(main_task pid=34262)[0m                       'tokenizer_path': '/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B',
[36m(main_task pid=34262)[0m                       'use_remove_padding': False},
[36m(main_task pid=34262)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=34262)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=34262)[0m                       'min_lr_ratio': None,
[36m(main_task pid=34262)[0m                       'total_training_steps': -1,
[36m(main_task pid=34262)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=34262)[0m             'ppo_epochs': 1,
[36m(main_task pid=34262)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=34262)[0m             'ppo_micro_batch_size': 2,
[36m(main_task pid=34262)[0m             'ppo_mini_batch_size': 8,
[36m(main_task pid=34262)[0m             'shuffle': False,
[36m(main_task pid=34262)[0m             'strategy': 'fsdp',
[36m(main_task pid=34262)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=34262)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=34262)[0m  'data': {'max_prompt_length': 256,
[36m(main_task pid=34262)[0m           'max_response_length': 512,
[36m(main_task pid=34262)[0m           'prompt_key': 'prompt',
[36m(main_task pid=34262)[0m /root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=34262)[0m No module named 'vllm._version'
[36m(main_task pid=34262)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=34678)[0m /root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=34678)[0m No module named 'vllm._version'
[36m(pid=34678)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=35016)[0m /root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=35016)[0m No module named 'vllm._version'
[36m(pid=35016)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=34678)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=34678)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=35016)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B and are newly initialized: ['score.bias']
[36m(WorkerDict pid=35016)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=34678)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B and are newly initialized: ['score.bias', 'score.weight']
[36m(main_task pid=34262)[0m           'return_raw_chat': False,
[36m(main_task pid=34262)[0m           'return_raw_input_ids': False,
[36m(main_task pid=34262)[0m           'tokenizer': None,
[36m(main_task pid=34262)[0m           'train_batch_size': 2,
[36m(main_task pid=34262)[0m           'train_files': 'data//train.parquet',
[36m(main_task pid=34262)[0m           'val_batch_size': 2,
[36m(main_task pid=34262)[0m           'val_files': 'data//test.parquet'},
[36m(main_task pid=34262)[0m  'reward_model': {'enable': False,
[36m(main_task pid=34262)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=34262)[0m                   'max_length': None,
[36m(main_task pid=34262)[0m                   'micro_batch_size': 64,
[36m(main_task pid=34262)[0m                   'model': {'external_lib': None,
[36m(main_task pid=34262)[0m                             'fsdp_config': {'min_num_params': 0,
[36m(main_task pid=34262)[0m                                             'param_offload': False},
[36m(main_task pid=34262)[0m                             'input_tokenizer': '/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B',
[36m(main_task pid=34262)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=34262)[0m                             'use_remove_padding': False},
[36m(main_task pid=34262)[0m                   'strategy': 'fsdp',
[36m(main_task pid=34262)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=34262)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=34262)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=34262)[0m              'default_hdfs_dir': None,
[36m(main_task pid=34262)[0m              'default_local_dir': 'checkpoints/TinyZero/countdown-qwen2.5-0.5b',
[36m(main_task pid=34262)[0m              'experiment_name': 'countdown-qwen2.5-0.5b',
[36m(main_task pid=34262)[0m              'logger': ['wandb'],
[36m(main_task pid=34262)[0m              'n_gpus_per_node': 2,
[36m(main_task pid=34262)[0m              'nnodes': 1,
[36m(main_task pid=34262)[0m              'project_name': 'TinyZero',
[36m(main_task pid=34262)[0m              'save_freq': 100,
[36m(main_task pid=34262)[0m              'test_freq': 100,
[36m(main_task pid=34262)[0m              'total_epochs': 15,
[36m(main_task pid=34262)[0m              'total_training_steps': None,
[36m(main_task pid=34262)[0m              'val_before_train': False}}
[36m(main_task pid=34262)[0m original dataset len: 327680
[36m(main_task pid=34262)[0m filter dataset len: 327680
[36m(main_task pid=34262)[0m original dataset len: 1024
[36m(main_task pid=34262)[0m filter dataset len: 1024
[36m(main_task pid=34262)[0m Size of train dataloader: 163840
[36m(main_task pid=34262)[0m Size of val dataloader: 1
[36m(main_task pid=34262)[0m Total training steps: 2457600
[36m(WorkerDict pid=34678)[0m Critic overriding config {'bos_token_id': None, 'eos_token_id': 151643, 'pad_token_id': 151643}
[36m(WorkerDict pid=34678)[0m Qwen2ForTokenClassification contains 1.54B parameters
[36m(WorkerDict pid=34678)[0m Before critic FSDP, memory allocated (GB): 0.0, memory reserved (GB): 0.0
[36m(WorkerDict pid=34678)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=34678)[0m After critic FSDP, memory allocated (GB): 2.8762574195861816, memory reserved (GB): 6.060546875
[36m(WorkerDict pid=34678)[0m Total steps: 2457600, num_warmup_steps: 0
[36m(WorkerDict pid=34678)[0m 
[36m(WorkerDict pid=34678)[0m 
[36m(WorkerDict pid=34678)[0m 
[36m(WorkerDict pid=34678)[0m \ critc {'strategy': 'fsdp', 'optim': {'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': 2457600}, 'model': {'path': '/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B', 'tokenizer_path': '/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B', 'override_config': {}, 'external_lib': None, 'enable_gradient_checkpointing': False, 'use_remove_padding': False, 'fsdp_config': {'param_offload': False, 'grad_offload': False, 'optimizer_offload': False, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': -1}}, 'ppo_mini_batch_size': 4, 'ppo_micro_batch_size': 1, 'forward_micro_batch_size': 1, 'use_dynamic_bsz': False, 'ppo_max_token_len_per_gpu': 32768, 'forward_max_token_len_per_gpu': 32768, 'ulysses_sequence_parallel_size': 1, 'ppo_epochs': 1, 'shuffle': False, 'grad_clip': 1.0, 'cliprange_value': 0.5}
[36m(WorkerDict pid=34678)[0m Critic use_remove_padding=False
[36m(WorkerDict pid=35016)[0m 
[36m(WorkerDict pid=35016)[0m 
[36m(WorkerDict pid=35016)[0m 
[36m(WorkerDict pid=34678)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=34678)[0m   "_name_or_path": "/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B",
[36m(WorkerDict pid=34678)[0m   "architectures": [
[36m(WorkerDict pid=34678)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=34678)[0m   ],
[36m(WorkerDict pid=34678)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=34678)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=34678)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=34678)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=34678)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=34678)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=34678)[0m   "max_position_embeddings": 131072,
[36m(WorkerDict pid=34678)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=34678)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=34678)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=34678)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=34678)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=34678)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=34678)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=34678)[0m   "rope_scaling": null,
[36m(WorkerDict pid=34678)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=34678)[0m   "sliding_window": null,
[36m(WorkerDict pid=34678)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=34678)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=34678)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=34678)[0m   "use_cache": true,
[36m(WorkerDict pid=34678)[0m   "use_mrope": false,
[36m(WorkerDict pid=34678)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=34678)[0m   "vocab_size": 151936
[36m(WorkerDict pid=34678)[0m }
[36m(WorkerDict pid=34678)[0m 
[36m(WorkerDict pid=34678)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=34678)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7fc5225f8af0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=34678)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=34678)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=34678)[0m   "_name_or_path": "/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B",
[36m(WorkerDict pid=34678)[0m   "architectures": [
[36m(WorkerDict pid=34678)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=34678)[0m   ],
[36m(WorkerDict pid=34678)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=34678)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=34678)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=34678)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=34678)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=34678)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=34678)[0m   "max_position_embeddings": 131072,
[36m(WorkerDict pid=34678)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=34678)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=34678)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=34678)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=34678)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=34678)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=34678)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=34678)[0m   "rope_scaling": null,
[36m(WorkerDict pid=34678)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=34678)[0m /root/miniconda3/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=34678)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=35016)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=35016)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=34678)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=34678)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(pid=gcs_server)[0m [2025-11-19 23:30:51,503 E 31385 31385] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[33m(raylet)[0m [2025-11-19 23:30:52,416 E 31694 31694] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(pid=31834)[0m [2025-11-19 23:30:52,874 E 31834 32211] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(WorkerDict pid=35016)[0m /root/miniconda3/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=35016)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=35016)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[2025-11-19 23:30:53,559 E 31340 31825] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(WorkerDict pid=34678)[0m /root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=34678)[0m   warnings.warn(
[36m(main_task pid=34262)[0m wandb: Currently logged in as: block0791 (block0791-alibaba) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=34262)[0m [2025-11-19 23:30:54,134 E 34262 34300] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14[32m [repeated 36x across cluster][0m
[36m(WorkerDict pid=35016)[0m /root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=35016)[0m   warnings.warn(
[36m(main_task pid=34262)[0m wandb: setting up run jlh0uhln
[36m(main_task pid=34262)[0m wandb: Tracking run with wandb version 0.23.0
[36m(main_task pid=34262)[0m wandb: Run data is saved locally in /root/TinyZero/wandb/run-20251119_233058-jlh0uhln
[36m(main_task pid=34262)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=34262)[0m wandb: Syncing run countdown-qwen2.5-0.5b
[36m(main_task pid=34262)[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/block0791-alibaba/TinyZero
[36m(main_task pid=34262)[0m wandb: üöÄ View run at https://wandb.ai/block0791-alibaba/TinyZero/runs/jlh0uhln
[36m(main_task pid=34262)[0m wandb: Detected [openai] in use.
[36m(main_task pid=34262)[0m wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[36m(main_task pid=34262)[0m wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
[36m(WorkerDict pid=34678)[0m *** SIGFPE received at time=1763566262 on cpu 121 ***
[36m(WorkerDict pid=34678)[0m PC: @     0x7fc57f8ff921  (unknown)  (unknown)
[36m(WorkerDict pid=34678)[0m     @     0x7fe527ebf520  (unknown)  (unknown)
[36m(WorkerDict pid=34678)[0m [2025-11-19 23:31:02,105 E 34678 34678] logging.cc:474: *** SIGFPE received at time=1763566262 on cpu 121 ***
[36m(WorkerDict pid=34678)[0m [2025-11-19 23:31:02,105 E 34678 34678] logging.cc:474: PC: @     0x7fc57f8ff921  (unknown)  (unknown)
[36m(WorkerDict pid=34678)[0m [2025-11-19 23:31:02,105 E 34678 34678] logging.cc:474:     @     0x7fe527ebf520  (unknown)  (unknown)
[36m(WorkerDict pid=34678)[0m Fatal Python error: Floating point exception
[36m(WorkerDict pid=34678)[0m 
[36m(WorkerDict pid=34678)[0m Stack (most recent call first):
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 40 in apply
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/model_executor/layers/logits_processor.py", line 83 in _get_logits
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/model_executor/layers/logits_processor.py", line 61 in forward
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562 in _call_impl
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553 in _wrapped_call_impl
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/model_executor/models/qwen2.py", line 424 in compute_logits
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/worker/model_runner.py", line 1698 in execute_model
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/worker/model_runner_base.py", line 116 in _wrapper
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116 in decorate_context
[36m(WorkerDict pid=34678)[0m   File "/root/TinyZero/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 267 in execute_model
[36m(WorkerDict pid=34678)[0m   File "/root/TinyZero/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 163 in execute_model
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 1386 in step
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/entrypoints/llm.py", line 879 in _run_engine
[36m(WorkerDict pid=34678)[0m   File "/root/TinyZero/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 161 in _run_engine
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/entrypoints/llm.py", line 353 in generate
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/utils.py", line 1063 in inner
[36m(WorkerDict pid=34678)[0m   File "/root/TinyZero/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 175 in generate_sequences
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116 in decorate_context
[36m(WorkerDict pid=34678)[0m   File "/root/TinyZero/verl/workers/fsdp_workers.py", line 419 in generate_sequences
[36m(WorkerDict pid=34678)[0m   File "/root/TinyZero/verl/single_controller/base/decorator.py", line 404 in inner
[36m(WorkerDict pid=34678)[0m   File "/root/TinyZero/verl/single_controller/ray/base.py", line 399 in func
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py", line 461 in _resume_span
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/function_manager.py", line 693 in actor_method_executor
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/worker.py", line 1042 in main_loop
[36m(WorkerDict pid=34678)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/workers/default_worker.py", line 322 in <module>
[36m(WorkerDict pid=35016)[0m 
[36m(WorkerDict pid=34678)[0m /root/miniconda3/envs/zero/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
[36m(WorkerDict pid=34678)[0m   warnings.warn('resource_tracker: There appear to be %d '
[36m(WorkerDict pid=34678)[0m   "sliding_window": null,
[36m(WorkerDict pid=34678)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=34678)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=34678)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=34678)[0m   "use_cache": true,
[36m(WorkerDict pid=34678)[0m   "use_mrope": false,
[36m(WorkerDict pid=34678)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=34678)[0m   "vocab_size": 151936
[36m(WorkerDict pid=34678)[0m }
[36m(WorkerDict pid=34678)[0m 
[36m(WorkerDict pid=34678)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=34678)[0m Before building vllm rollout, memory allocated (GB): 7.211911678314209, memory reserved (GB): 10.4140625
[36m(WorkerDict pid=34678)[0m INFO 11-19 23:30:48 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=34678)[0m WARNING 11-19 23:30:48 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=35016)[0m Total steps: 2457600, num_warmup_steps: 0[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=35016)[0m \ critc {'strategy': 'fsdp', 'optim': {'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': 2457600}, 'model': {'path': '/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B', 'tokenizer_path': '/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B', 'override_config': {}, 'external_lib': None, 'enable_gradient_checkpointing': False, 'use_remove_padding': False, 'fsdp_config': {'param_offload': False, 'grad_offload': False, 'optimizer_offload': False, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': -1}}, 'ppo_mini_batch_size': 4, 'ppo_micro_batch_size': 1, 'forward_micro_batch_size': 1, 'use_dynamic_bsz': False, 'ppo_max_token_len_per_gpu': 32768, 'forward_max_token_len_per_gpu': 32768, 'ulysses_sequence_parallel_size': 1, 'ppo_epochs': 1, 'shuffle': False, 'grad_clip': 1.0, 'cliprange_value': 0.5}
[36m(WorkerDict pid=35016)[0m Critic use_remove_padding=False
[36m(WorkerDict pid=35016)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7ef6d27f9af0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=35016)[0m Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=34678)[0m local rank 0
[36m(WorkerDict pid=34678)[0m INFO 11-19 23:30:48 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=34678)[0m INFO 11-19 23:30:48 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=34678)[0m INFO 11-19 23:30:48 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=34678)[0m INFO 11-19 23:30:48 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc1e230ffa0>, local_subscribe_port=56725, remote_subscribe_port=None)
[36m(WorkerDict pid=34678)[0m before init cache memory allocated: 9.358577664GB, reserved: 9.4896128GB
[36m(WorkerDict pid=34678)[0m after init cache memory allocated: 44.532011008GB, reserved: 44.663046144GB
[36m(WorkerDict pid=34678)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 512, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=34678)[0m After building vllm rollout, memory allocated (GB): 40.032227516174316, memory reserved (GB): 41.595703125
[36m(WorkerDict pid=34678)[0m After building sharding manager, memory allocated (GB): 40.032227516174316, memory reserved (GB): 41.595703125
[36m(WorkerDict pid=35016)[0m INFO 11-19 23:30:48 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=35016)[0m WARNING 11-19 23:30:48 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=35016)[0m local rank 0
[36m(WorkerDict pid=35016)[0m INFO 11-19 23:30:48 selector.py:115] Using XFormers backend.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=35016)[0m INFO 11-19 23:30:48 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=35016)[0m INFO 11-19 23:30:48 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(main_task pid=34262)[0m epoch 0, step 1
[36m(WorkerDict pid=35016)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 512, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. Lease ID: 00000000e488f183be3317ca4bc7356030e6b118086817b1ec66c5bed6555845 Worker ID: cac00f07193eb3872851b27e42ece04b19054ced5eb5c3161f8fcfa6 Node ID: 0df0160df7f5113d42839fc5e7c6f4472891b993875fa538d4bd4ec0 Worker IP address: 172.17.0.3 Worker port: 38095 Worker PID: 34678 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
Error executing job with overrides: ['data.train_files=data//train.parquet', 'data.val_files=data//test.parquet', 'data.train_batch_size=2', 'data.val_batch_size=2', 'data.max_prompt_length=256', 'data.max_response_length=512', 'actor_rollout_ref.model.path=/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.use_dynamic_bsz=False', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.actor.ppo_mini_batch_size=8', 'actor_rollout_ref.actor.ppo_micro_batch_size=2', 'actor_rollout_ref.rollout.log_prob_micro_batch_size=1', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.4', 'actor_rollout_ref.ref.log_prob_micro_batch_size=1', 'critic.optim.lr=1e-5', 'critic.model.path=/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B', 'critic.ppo_micro_batch_size=2', 'critic.ppo_mini_batch_size=8', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.logger=[wandb]', '+trainer.val_before_train=False', 'trainer.default_hdfs_dir=null', 'trainer.n_gpus_per_node=2', 'trainer.nnodes=1', 'trainer.save_freq=100', 'trainer.test_freq=100', 'trainer.project_name=TinyZero', 'trainer.experiment_name=countdown-qwen2.5-0.5b', 'trainer.total_epochs=15']
Traceback (most recent call last):
  File "/root/TinyZero/verl/trainer/main_ppo.py", line 103, in main
    ray.get(main_task.remote(config))
  File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
  File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ActorDiedError): [36mray::main_task()[39m (pid=34262, ip=172.17.0.3)
  File "/root/TinyZero/verl/trainer/main_ppo.py", line 189, in main_task
    trainer.fit()
  File "/root/TinyZero/verl/trainer/ppo/ray_trainer.py", line 589, in fit
    gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)
  File "/root/TinyZero/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.
	class_name: create_colocated_worker_cls.<locals>.WorkerDict
	actor_id: f7bedc181a1db1b913f21e4301000000
	pid: 34678
	name: xXw2jQWorkerDict_0:0
	namespace: c51af065-d8f5-47d5-a872-4dbb3ef1e719
	ip: 172.17.0.3
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(WorkerDict pid=34678)[0m [2025-11-19 23:31:00,578 E 34678 34760] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=35016)[0m *** SIGFPE received at time=1763566262 on cpu 139 ***
[36m(WorkerDict pid=35016)[0m PC: @     0x7ef72fcff921  (unknown)  (unknown)
[36m(WorkerDict pid=35016)[0m     @     0x7f16d80c4520  (unknown)  (unknown)
[36m(WorkerDict pid=35016)[0m [2025-11-19 23:31:02,101 E 35016 35016] logging.cc:474: *** SIGFPE received at time=1763566262 on cpu 139 ***
[36m(WorkerDict pid=35016)[0m [2025-11-19 23:31:02,101 E 35016 35016] logging.cc:474: PC: @     0x7ef72fcff921  (unknown)  (unknown)
[36m(WorkerDict pid=35016)[0m [2025-11-19 23:31:02,101 E 35016 35016] logging.cc:474:     @     0x7f16d80c4520  (unknown)  (unknown)
[36m(WorkerDict pid=35016)[0m Fatal Python error: Floating point exception
[36m(WorkerDict pid=35016)[0m Stack (most recent call first):
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 40 in apply
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/model_executor/layers/logits_processor.py", line 83 in _get_logits
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/model_executor/layers/logits_processor.py", line 61 in forward
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562 in _call_impl
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553 in _wrapped_call_impl
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/model_executor/models/qwen2.py", line 424 in compute_logits
[36m(WorkerDict pid=35016)[0m   File "/root/TinyZero/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 163 in execute_model[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/worker/model_runner_base.py", line 116 in _wrapper
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116 in decorate_context[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 1386 in step
[36m(WorkerDict pid=35016)[0m   File "/root/TinyZero/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 161 in _run_engine[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/entrypoints/llm.py", line 353 in generate
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/vllm/utils.py", line 1063 in inner
[36m(WorkerDict pid=35016)[0m   File "/root/TinyZero/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 175 in generate_sequences
[36m(WorkerDict pid=35016)[0m   File "/root/TinyZero/verl/workers/fsdp_workers.py", line 419 in generate_sequences
[36m(WorkerDict pid=35016)[0m   File "/root/TinyZero/verl/single_controller/base/decorator.py", line 404 in inner
[36m(WorkerDict pid=35016)[0m   File "/root/TinyZero/verl/single_controller/ray/base.py", line 399 in func
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py", line 461 in _resume_span
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/function_manager.py", line 693 in actor_method_executor
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/worker.py", line 1042 in main_loop
[36m(WorkerDict pid=35016)[0m   File "/root/miniconda3/envs/zero/lib/python3.9/site-packages/ray/_private/workers/default_worker.py", line 322 in <module>
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. Lease ID: 02000000d79087928757cd0da146b7522f888c23a0e4d967dbf62cd04165d94e Worker ID: e473cf995242cba8e203e36b214e735acb60c1894be66700dbe137c5 Node ID: 0df0160df7f5113d42839fc5e7c6f4472891b993875fa538d4bd4ec0 Worker IP address: 172.17.0.3 Worker port: 45495 Worker PID: 35016 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
